import pandas as pd
import os
from utils import *
from sklearn.preprocessing import LabelEncoder
import joblib
import numpy as np

from sklearn.model_selection import StratifiedKFold, cross_val_score
from xgboost import XGBClassifier

seed_everything(22520465)

root_folder = 'output_true'
model_folders = [f for f in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, f))]
train_file = 'dev_predictions_with_probs.csv'
test_file = 'submit_with_probs_privatetest.csv'

train_meta = feature_engineering(root_folder, model_folders, file=train_file)

test_meta = feature_engineering(root_folder, model_folders, file=test_file, train=False)


# ƒê·ªçc d·ªØ li·ªáu nh√£n
train_df = pd.read_csv('data/train_dsc.csv')  # ch·ª©a 'id' v√† 'label' (chu·ªói)

# Encode nh√£n (chu·ªói -> s·ªë nguy√™n)
le = LabelEncoder()
train_df['label_encoded'] = le.fit_transform(train_df['label'])

# G·ªôp v·ªõi train_meta theo 'id' ƒë·ªÉ t·∫°o X, y t∆∞∆°ng ·ª©ng
merged_df = pd.merge(train_meta, train_df[['id', 'label_encoded']], on='id')

# T√°ch X v√† y
X = merged_df.drop(columns=['id', 'label_encoded'])
y = merged_df['label_encoded']



# ==========================
# 1. CV c·ªë ƒë·ªãnh (n·∫øu b·∫°n c√≤n c·∫ßn d√πng ƒë·ªÉ ƒë√°nh gi√° th·ªß c√¥ng)
# ==========================
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=22520465)

# ==========================
# 2. Tham s·ªë ƒë√£ bi·∫øt
# ==========================
best_params = {
    'colsample_bytree': 0.8,
    'gamma': 0,
    'learning_rate': 0.01,
    'max_depth': 3,
    'n_estimators': 500,
    'subsample': 1,
    'tree_method': 'hist',
    'device': 'cuda',
    'eval_metric': 'mlogloss',
    'random_state': 22520465,
    'n_jobs': -1
}

# ==========================
# 3. Train m√¥ h√¨nh
# ==========================
best_model = XGBClassifier(**best_params)
best_model.fit(X, y)

# ==========================
# 4. (Tu·ª≥ ch·ªçn) ƒê√°nh gi√° CV n·∫øu c·∫ßn ki·ªÉm tra l·∫°i F1 macro
# ==========================
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score

score = cross_val_score(
    best_model, X, y,
    cv=cv,
    scoring='f1_macro',
    n_jobs=-1
).mean()

print(f"‚úÖ F1 Macro CV Score: {score:.4f}")

# üëâ Sau khi train xong b·∫°n c√≥ th·ªÉ l∆∞u best_model b·∫±ng pickle ho·∫∑c joblib
joblib.dump(best_model, "xgb_best_model.pkl")

# L·∫•y ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o X_test (lo·∫°i b·ªè c·ªôt id)
X_test = test_meta.drop(columns=['id'])

# D·ª± ƒëo√°n nh√£n d·∫°ng s·ªë
y_test_pred = best_model.predict(X_test)

# L·∫•y l·∫°i t√™n nh√£n t∆∞∆°ng ·ª©ng
y_test_label = le.inverse_transform(y_test_pred)

# G·ªôp id v√† nh√£n d·ª± ƒëo√°n
submit_df = pd.DataFrame({
    'id': test_meta['id'],
    'predict_label': y_test_label
})

# L∆∞u file CSV
submit_df.to_csv('submit.csv', index=False)